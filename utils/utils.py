import glob
import json
import os
import os.path as os_path
import torch
import numpy as np
import torchaudio.transforms as T
import librosa
import torchaudio
import torchaudio.transforms as T
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt

from utils.frame import VocalData, load_vocals, freq_to_note

class AttrDict(dict):
    def __init__(self, *args, **kwargs):
        super(AttrDict, self).__init__(*args, **kwargs)
        self.__dict__ = self

def get_config(config_file):
    with open(config_file) as f:
        data = f.read()
    json_config = json.loads(data)
    return AttrDict(json_config)

def plot_spectrograms(wave, new_wave, title_wave="tr"):
    plt.figure(figsize=(10, 5))  # –°–æ–∑–¥–∞—ë–º –æ–¥–Ω–æ –æ–∫–Ω–æ –¥–ª—è –æ–±–æ–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤

    # –ü–µ—Ä–≤–∞—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
    plt.subplot(2, 1, 1)
    plt.imshow(wave, aspect='auto', origin='lower')
    plt.colorbar(label="–ê–º–ø–ª–∏—Ç—É–¥–∞")
    plt.xlabel("–§—Ä–µ–π–º—ã (–≤—Ä–µ–º—è)")
    plt.ylabel("–ú–µ–ª-—á–∞—Å—Ç–æ—Ç—ã")
    plt.title(f"–û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ ({title_wave})")

    # –í—Ç–æ—Ä–∞—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
    plt.subplot(2, 1, 2)
    plt.imshow(new_wave, aspect='auto', origin='lower')
    plt.colorbar(label="–ê–º–ø–ª–∏—Ç—É–¥–∞")
    plt.xlabel("–§—Ä–µ–π–º—ã (–≤—Ä–µ–º—è)")
    plt.ylabel("–ú–µ–ª-—á–∞—Å—Ç–æ—Ç—ã")
    plt.title("–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –º–µ–ª-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞")

    plt.tight_layout()  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥–≥–æ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
    plt.show()


def plot_spectrograms__(waves, labels):
    plt.figure(figsize=(10, 5))  # –°–æ–∑–¥–∞—ë–º –æ–¥–Ω–æ –æ–∫–Ω–æ –¥–ª—è –æ–±–æ–∏—Ö –≥—Ä–∞—Ñ–∏–∫–æ–≤

    # –ü–µ—Ä–≤–∞—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞
    for idx, w in enumerate(waves,):
        plt.subplot(len(waves), 1, idx+1)
        plt.imshow(w, aspect='auto', origin='lower')
        plt.colorbar(label="–ê–º–ø–ª–∏—Ç—É–¥–∞")
        plt.xlabel("–§—Ä–µ–π–º—ã (–≤—Ä–µ–º—è)")
        plt.ylabel("–ú–µ–ª-—á–∞—Å—Ç–æ—Ç—ã")
        plt.title(labels[idx])

    plt.tight_layout()  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–æ–¥–≥–æ–Ω–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
    plt.show()


    # üîπ –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º
def plot_spectrogram(mel_spec, title):
    plt.figure(figsize=(10, 4))
    plt.imshow(mel_spec.log2().detach().cpu().numpy(), aspect="auto", origin="lower")
    plt.colorbar(label="Log Amplitude")
    plt.title(title)
    plt.xlabel("Time Frames")
    plt.ylabel("Mel Bands")
    plt.show()

# üîπ –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ç—Ä–∏—Å–æ–≤–∫–∏ waveforms
def plot_waveform(waveform1, title, sample_rate=16000):
    plt.figure(figsize=(10, 3))
    time_axis = np.linspace(0, len(waveform1) / sample_rate, num=len(waveform1))
    plt.plot(time_axis, waveform1, linewidth=0.8, color="green")
    plt.title(title)
    plt.xlabel("Time (s)")
    plt.ylabel("Amplitude")
    plt.grid()
    plt.show()

# üîπ –§—É–Ω–∫—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
def plot_waveforms(orig_wave, new_wave):
    plt.figure(figsize=(12, 6))

    # –û—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å–∏–≥–Ω–∞–ª
    plt.plot(orig_wave, label="–û—Ä–∏–≥–∏–Ω–∞–ª", color="green")
    plt.plot(new_wave, label="–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π", color="red", alpha=0.7)
    plt.title("–í—Ä–µ–º–µ–Ω–Ω–æ–π —Å–∏–≥–Ω–∞–ª")
    plt.legend()

    plt.show()


def scan_checkpoint(cp_dir, prefix):
    pattern = os.path.join(cp_dir, prefix + '????????')
    cp_list = glob.glob(pattern)
    if len(cp_list) == 0:
        return None
    return sorted(cp_list)[-1]

def save_checkpoint(filepath, obj):
    print("Saving checkpoint to {}".format(filepath))
    torch.save(obj, filepath)
    print("Complete.")

def graph(ds, colors = "orange"):
    if not isinstance(ds, list):
        ds = [ds]
    if not isinstance(colors, list):
        colors = [colors]
    for d, color in zip(ds, colors):
        plt.plot(np.arange(len(d)), d, color=color)
    plt.show()

def load_checkpoint(filepath, device):
    print("Loading '{}'".format(filepath))
    checkpoint_dict = torch.load(filepath, map_location=device)
    print("Complete.")
    return checkpoint_dict

def prepare_and_save_checkpoints(checkpoint_path, generator, steps, epoch):
    save_checkpoint(
        "{}/tr_{:08d}".format(checkpoint_path, steps), 
        {'generator': generator.state_dict(), "steps":steps, "epoch": epoch}
    )

def files_with_type_fiter(path, type, need_join=True):
        return [
            os_path.join(path, f) if need_join else f
            for f in os.listdir(path)
            if os_path.isfile(os_path.join(path, f)) and f.endswith(type)
        ]

# export ROCM_PATH=/opt/rocm
def load_wavs_f0(ideals: list[VocalData], framesamp, max_hz, sr=16000):
    files_dict = {}
    for w in ideals:
        raw_data, sample_rate = torchaudio.load(w.file_name)
                            # –†–µ—Å—ç–º–ø–ª–∏–º –≤ 16–∫–ì—Ü, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if sr != sample_rate:
            resampler = T.Resample(orig_freq=sample_rate, new_freq=sr)
            waveform = resampler(raw_data)

        waveform = torchaudio.functional.lowpass_biquad(waveform, sr, max_hz)
        num_chunks = waveform.shape[1] // framesamp
        files_dict[w.file_name] = waveform[:, :num_chunks * framesamp].reshape(
                num_chunks, framesamp
        ).squeeze(0)

    return files_dict


def load_l0_vocals(path, frames, hop, min_hz, max_hz, sr=16000):
        #files = ["./data/ideals/1.wav", "./data/ideals/2.wav", "./data/ideals/3.wav"]
        #files = ["./data/ideal2.wav", "./data/c_ideal2.wav"]

    files = files_with_type_fiter(path, ".wav", True)
    ideals = load_vocals(files, frames, hop, min_hz, max_hz, sr=sr)
    torch_files = load_wavs_f0(ideals, frames, max_hz, sr)


    min_note, max_note = freq_to_note(min_hz), freq_to_note(max_hz)
    ideal_dict = {i: [] for i in range(min_note, max_note + 1)}

    for ideal in ideals:
        for idx, frame in enumerate(ideal.frames):
            if not frame.training_freq or ideal_dict.get(frame._note) is None:
                continue
            ideal_dict[frame._note].append(torch_files[ideal.file_name].data[idx].tolist())

    for key, item in ideal_dict.items():
        if not item:
            print(f"{key} - –Ω–µ –∏–º–µ–µ—Ç –¥–∞–Ω–Ω—ã—Ö")

    return ideal_dict


def detect_f0(file, framesamp, hop, min_hz, max_hz, sr, n_steps):
    import scipy.io.wavfile as wav
    sample_rate, raw_data = wav.read(file)
    from resampy import resample
    raw_data = resample(raw_data, sample_rate, sr)

    y_shifted = librosa.effects.pitch_shift(raw_data, sr=sample_rate, n_steps=n_steps)
    vocal = VocalData(
        file, framesamp, hop, min_hz, max_hz, model_rate=sr, vocal=y_shifted, vocal_rate=sr
    )
    vocal.load()
    return vocal.get_freqs()


pitch_shift = None
def shift(data, from_freq, to_freq, sr, device):
    global pitch_shift
    if not pitch_shift:
        pitch_shift = T.PitchShift(sr, 1).to(device)
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–¥–≤–∏–≥–∞ –ø–æ –≤—ã—Å–æ—Ç–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞
    for i in range(len(from_freq)):
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É —á–∞—Å—Ç–æ—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—Ç–æ–Ω–æ–≤
        n_steps = 12 * np.log2(to_freq[i] / from_freq[i])
        pitch_shift.n_steps = round(n_steps)
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ PitchShift
        #pitch_shift = T.PitchShift(sr, n_steps=round(n_steps.item())).to(device)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .item() –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –æ–±—ã—á–Ω–æ–µ —á–∏—Å–ª–æ
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º torch.no_grad(), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        with torch.no_grad():
            # –ü—Ä–∏–º–µ–Ω—è–µ–º PitchShift –∫ —Å–∏–≥–Ω–∞–ª—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–π inplace
            data[i] = pitch_shift(data[i].clone())  # clone() –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç inplace-–∏–∑–º–µ–Ω–µ–Ω–∏—è
    
    return data

def shift_by_steps(data, steps, sr, device):
    pitch_shift = T.PitchShift(sr, 1).to(device)
    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å–¥–≤–∏–≥–∞ –ø–æ –≤—ã—Å–æ—Ç–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–ª–æ–∫–∞
    for i in range(len(steps)):
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω–∏—Ü—É —á–∞—Å—Ç–æ—Ç –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—Ç–æ–Ω–æ–≤
        n_steps = steps[i]
        pitch_shift.n_steps = round(n_steps)
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ PitchShift
        #pitch_shift = T.PitchShift(sr, n_steps=round(n_steps.item())).to(device)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .item() –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤ –æ–±—ã—á–Ω–æ–µ —á–∏—Å–ª–æ
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º torch.no_grad(), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–ø–µ—Ä–∞—Ü–∏–π –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        with torch.no_grad():
            # –ü—Ä–∏–º–µ–Ω—è–µ–º PitchShift –∫ —Å–∏–≥–Ω–∞–ª—É, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏–π inplace
            data[i] = pitch_shift(data[i].clone())  # clone() –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç inplace-–∏–∑–º–µ–Ω–µ–Ω–∏—è
    
    return data

def shift_full(data, step, sr, device):
    pitch_shift = T.PitchShift(sr, step).to(device)
    with torch.no_grad():
        return pitch_shift(data)